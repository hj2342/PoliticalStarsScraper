Web scraping needs to be done responsibly, and we’ve taken care to ensure our project respects these boundaries:

1- Respecting robots.txt: Before starting the scraping process, we checked Times of India’s robots.txt file to ensure we weren’t violating their rules. It’s crucial to ensure you’re not scraping pages that are off-limits.

2- Rate Limiting: Although this is a small project and only scrapes one page, for larger-scale projects, it’s essential to include rate limiting to avoid overwhelming the server.

3- Data Usage: The scraped data is used solely for educational purposes and in compliance with fair use. We extract publicly available information from a single page without infringing on any terms of service.